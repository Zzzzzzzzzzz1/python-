# 2.回归问题
回归问题在形式上与线性分类问题十分相似
但是在分类问题中预测值y是一个离散的变量。
它代表着通过特征x所预测出来的类别，而在回归问题中，y是一个连续的变量
logistic回归：y是一个虚拟变量，即y只能等于0或者1，通过这种方法做样本之间的划分。
线性回归则是y与x有一个类似于函数的关系，但是这种函数并不是严格对应的，常用的方法是最小二乘法
## 2.1我们可以使用pytorch来实现logistic回归
### 2.1.1数据准备
```python
import matplotlib
import torch
from torch import nn
from matplotlib import pyplot as plt #数据可视化
import numpy as np
from torch.distributions import MultivariateNormal
#logistic回归常用于解决二分类问题
#这里我们采用引入两个多元高斯分布，多元高斯分布的意思就是一个tuple（x,y），x,y
#都满足一个正态分布，把这样的正态分布可以画在一张散点图中
#以下我们设置两个高斯分布的均值向量和协方差矩阵,作为数据准备
mu1= -3*torch.ones(2)
mu2=  3*torch.ones(2)
#这里的sigama是协方差矩阵，对角线为变量的方差，其余为两个变量的协方差
sigma1= torch.eye(2)*0.5
sigma2= torch.eye(2)*2
#从两个多元高斯分布中生成100个样本
m1=MultivariateNormal(mu1,sigma1)
m2=MultivariateNormal(mu2,sigma2)
x1=m1.sample((100,))
x2=m2.sample((100,))
#设置正负样本的标签
y=torch.zeros((200,1))
y[100:]=1  #后100个y为1
#组合、打乱样本
x=torch.cat([x1,x2],dim=0)
idx=np.random.permutation(len(x)) #生成200个整数作为下标
#这里其实打不打乱没有区别
x=x[idx]
y=y[idx]
#绘制样本
plt.scatter(x1.numpy()[:,0],x1.numpy()[:,1])
plt.scatter(x2.numpy()[:,0],x2.numpy()[:,1])
```
### 2.1.2 线性方程
```python
D_in,D_out=2,1  #定义了线性模型的输入维度和输出维度
linear=nn.Linear(D_in,D_out,bias=True)
output=linear(x)
print(x.shape,linear.weight.shape,linear.bias.shape,output.shape) 
def my_linear(x,w,b):
    return torch.mm(x,w.t())+b
torch.sum((output-my_linear(x,linear.weight,linear.bias)))
```
### 2.1.3 激活函数
```python
sigmoid=nn.Sigmoid
scores=sigmoid(output)
def my_sigmoid(x):
    x=1/(1+torch.exp(-x))
    return x
    torch.sum(sigmoid(output)-sigmoid_(output))
```
### 2.1.4 损失函数
```python
loss=nn.BCELoss()
loss(sigmoid(output),y)
def my_loss(x,y):
    loss=-torch.mean(torch.log(x)*y+torch.log(1-x)*(1-y))
    return loss
import torch.nn as nn
class LogisticRegression(nn.Module):
    def __init__(self,D_in):
        super(LogisticRegression,self).__init__()
        self.linear=nn.Linear(D_in,1)
        self.sigmoid=nn.Sigmoid()
    def forward(self,x):
        x=self.linear(x)
        output=self.sigmoid(x)
        return output
lr_model=LogisticRegression(2)
loss=nn.BCELoss()
print(loss(lr_model(x),y))
```
### 2.1.5 优化算法
```python
from torch import optim
optimizer=optim.SGD(lr_model.parameters(),lr=0.03)
barch_size=10
iters=10
# for input,target in dataset:
for _ in range(iters):
    for i in range(int(len(x)/batch_size)):
        input=x[i*batch_size:(i+1)*batch_size]
        target=y[i * batch_size:(i+1)*batch_size]
        optimizer.zero_grad()
        output=lr_model(input)
        l=loss(output,target)
        l.backward()
        optimizer.step
```
### 2.1.6 模型可视化
```python
pred_neg=(output<=0.5).view(-1)
pred_pos=(output>0.5).view(-1)
plt.scatter(x[pred_neg,0],x[pred_neg,1])
plt.scatter(x[pred_pos,0],x[pred_pos,1])
w=lr_model.linear.weight[0]
b=lr_model.linear.bias[0]
def draw_decision_boundary(w,b,x0):
    x1=(-b-w[0]*x0)/w[1]
    plt.plot(x0.detach().numpy(),x1.detach().numpy(),'r')
    plt.show()
draw_decision_boundary(w,b,torch.linspace(x.min(),x.max(),50))
```



